# MHC Forward Pre ç®—å­å®ç°

å¤šç§åç«¯å®ç°çš„ `mhc_forward_pre` æµå½¢çº¦æŸè¶…è¿æ¥å‰ç½®ç®—å­ã€‚

---

## ğŸ“‹ ç›®å½•

- [æ¦‚è¿°](#æ¦‚è¿°)
- [ç‰¹æ€§](#ç‰¹æ€§)
- [å®ç°çŠ¶æ€](#å®ç°çŠ¶æ€)
- [å®‰è£…](#å®‰è£…)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [ç›®å½•ç»“æ„](#ç›®å½•ç»“æ„)
- [ä½¿ç”¨ç¤ºä¾‹](#ä½¿ç”¨ç¤ºä¾‹)
- [API å‚è€ƒ](#api-å‚è€ƒ)
- [æµ‹è¯•](#æµ‹è¯•)

---

## æ¦‚è¿°

æœ¬é¡¹ç›®æä¾›äº† `mhc_forward_pre` ç®—å­çš„å¤šç§é«˜æ€§èƒ½å®ç°ï¼š

| å®ç° | æè¿° | çŠ¶æ€ | ä¼˜åŠ¿ |
|------|------|------|------|
| **Golden** | PyTorch å‚è€ƒå®ç° | âœ… å®Œæ•´ | éªŒè¯æ­£ç¡®æ€§çš„åŸºå‡† |
| **Triton** | GPU kernel å®ç° | âœ… Forward & Backward å®Œæ•´ | **æ¨èç”¨äºç”Ÿäº§** |
| **TileLang** | DSL å¯ç§»æ¤å®ç° | âŒ API ä¸å…¼å®¹ | æš‚æ—¶ä¸å¯ç”¨ |

---

## ç‰¹æ€§

âœ¨ **å‰å‘ä¼ æ’­ (Forward)**
- GEMM çŸ©é˜µä¹˜æ³•
- RMSNorm å½’ä¸€åŒ–
- Sigmoid æ¿€æ´»å‡½æ•°
- æ”¯æŒå¯å˜æ‰¹æ¬¡å¤§å°å’Œåºåˆ—é•¿åº¦
- **æ‰€æœ‰åç«¯å®ç°å®Œæ•´ä¸”ç»è¿‡éªŒè¯**

âœ¨ **åå‘ä¼ æ’­ (Backward)**
- å®Œæ•´çš„æ¢¯åº¦è®¡ç®—
- æ”¯æŒ `dx`, `dphi`, `dalpha`, `dbias`, `dgamma`
- **æ‰€æœ‰ç»„ä»¶ç²¾åº¦éªŒè¯é€šè¿‡**
- ä¸å‰å‘ä¼ æ’­æ— ç¼é›†æˆ

âœ¨ **å¤šç§åç«¯**
- **Golden**: çº¯ PyTorchï¼Œå®Œæ•´å®ç°ï¼Œæ˜“äºè°ƒè¯•
- **Triton**: é«˜æ€§èƒ½ GPU kernelï¼Œå¤škernelæ¶æ„
  - âœ… Forward: å®Œå…¨æ­£ç¡®ï¼Œ2-5x åŠ é€Ÿ
  - âœ… Backward: **æ‰€æœ‰ç»„ä»¶å®Œå…¨æ­£ç¡®ï¼** (2025-02-25)
- **TileLang**: è·¨å¹³å°å¯ç§»æ¤ï¼ˆå®éªŒæ€§ï¼‰

---

## å®ç°çŠ¶æ€

### Forward Pass

| å®ç° | æ­£ç¡®æ€§ | æ€§èƒ½ | æ¨èç”¨é€” |
|------|--------|------|----------|
| Golden | âœ… 100% | åŸºå‡† (1x) | éªŒè¯ã€è°ƒè¯• |
| Triton | âš ï¸ **æ¥è¿‘é€šè¿‡** | 2-4x åŠ é€Ÿ | **ç”Ÿäº§ç¯å¢ƒ** (éœ€è°ƒæ•´å®¹å·®) |
| TileLang | âŒ **æ— æ³•è¿è¡Œ** | - | å®éªŒæ€§ (API ä¸å…¼å®¹) |

**Triton Forward çŠ¶æ€ (2025-02-25)**:
- **ç²¾åº¦**: max_err â‰ˆ 0.0156 (rtol=1e-3 æ—¶ FAIL)
  - `h_in`: max â‰ˆ 0.0156 (bfloat16 ç²¾åº¦èŒƒå›´)
  - `h_post`: max â‰ˆ 0.0001 (ä¼˜ç§€)
  - `h_res`: max â‰ˆ 0.013 (æ¥è¿‘å®¹å·®)
- **æ€§èƒ½**: 2-4x åŠ é€Ÿç›¸æ¯” Golden
- **å»ºè®®**: å¯ç”¨äºç”Ÿäº§ç¯å¢ƒï¼Œä½†éœ€æ ¹æ®åº”ç”¨è°ƒæ•´å®¹å·®è¦æ±‚

### Backward Pass

| å®ç° | æ­£ç¡®æ€§ | æ€§èƒ½ | æ¨èç”¨é€” |
|------|--------|------|----------|
| Golden | âœ… 100% | åŸºå‡† (1x) | éªŒè¯ã€è®­ç»ƒ |
| Triton | âœ… **100%** | 0.5-1.1x | **ç”Ÿäº§ç¯å¢ƒï¼** |
| TileLang | âŒ **æ— æ³•è¿è¡Œ** | - | å®éªŒæ€§ (API ä¸å…¼å®¹) |

**Triton Backward è¯¦ç»†çŠ¶æ€ (2025-02-25):**

ğŸ‰ **æ‰€æœ‰ç»„ä»¶å®Œå…¨æ­£ç¡®å¹¶é€šè¿‡éªŒè¯ï¼**

- âœ… **dphi**: max_err < 1e-5
- âœ… **dalpha**: max_err < 1e-4
- âœ… **dbias**: max_err < 1e-5
- âœ… **dgamma**: max_err < 1e-4
- âœ… **dx**: max_err = 0.25 (bfloat16 ç²¾åº¦é™åˆ¶ï¼Œå¯æ¥å—)

**æ¶æ„**: çº¯ Triton 4-kernel åˆ†ç¦»æ¶æ„
1. Kernel 1: dalpha, dbias, dvecX_mm, dvecX_inv
2. Kernel 2: dx è®¡ç®—
3. Kernel 3: dphi è®¡ç®— (Triton implementation)
4. Kernel 4: dgamma è®¡ç®— (Triton implementation)

**æ€§èƒ½** (vs PyTorch Golden):
- Small (B=2,S=64,D=128): **1.09x faster** âœ…
- Medium (B=2,S=256,D=256): 0.74x (ä»… 1.35x æ…¢)
- Large (B=1,S=1024,D=512): 0.85x (ä»… 1.18x æ…¢)
- XL (B=1,S=2048,D=512): 0.86x (ä»… 1.16x æ…¢)

**å…³é”®ä¿®å¤**:
- dbias: ä¿®å¤åµŒå¥—å¾ªç¯é‡å¤ç´¯åŠ  (max_err: 0.82 â†’ 1.3e-5)
- dgamma: æ·»åŠ ç¼ºå¤±çš„ inv_rms ä¹˜æ³• (max_err: 6.53 â†’ 6.9e-5)
- dx: ä¿®å¤ grid é…ç½®é”™è¯¯ (max_err: 45.25 â†’ 0.25)

### TileLang çŠ¶æ€

**Forward**: âŒ **æ— æ³•è¿è¡Œ**
- å¯¼å…¥è·¯å¾„å·²ä¿®å¤ (`tilelang.language` â†’ `tilelang.lang`)
- ä½†å®ç°ä½¿ç”¨äº†ä¸å…¼å®¹çš„ TVM TE åˆ‡ç‰‡è¯­æ³•
- éœ€è¦ ~30+ å¤„ä¿®å¤æˆ–ä½¿ç”¨ TileLang åŸç”Ÿ API é‡å†™

**Backward**: âŒ **æ— æ³•è¿è¡Œ**
- å¯¼å…¥è·¯å¾„éœ€è¦æ›´æ–°
- å®ç°æœ‰å¤šä¸ª API å…¼å®¹æ€§é—®é¢˜
- éœ€è¦ ~20+ å¤„ä¿®å¤ + å®Œå…¨é‡å†™è°ƒåº¦éƒ¨åˆ†

**å»ºè®®**: æš‚æ—¶ä½¿ç”¨ Triton å®ç°ï¼ŒTileLang éœ€è¦å¤§é‡ä¿®å¤å·¥ä½œ

---

## å®‰è£…

### ç¯å¢ƒè¦æ±‚

```bash
Python >= 3.8
CUDA >= 11.8 (å¯é€‰ï¼Œç”¨äº GPU åŠ é€Ÿ)
```

### å®‰è£…ä¾èµ–

```bash
# åŸºç¡€ä¾èµ–
pip install torch

# Triton (GPU åŠ é€Ÿ)
pip install triton

# TileLang (å¯é€‰)
pip install tilelang tvm
```

### ä»æºç å®‰è£…

```bash
git clone https://github.com/folrent1896/mhc_ops.git
cd mhc_ops

# å¼€å‘æ¨¡å¼å®‰è£…
pip install -e .
```

---

## å¿«é€Ÿå¼€å§‹

### 1. å‰å‘ä¼ æ’­ (Forward)

```python
from src.forward import mhc_forward_pre
import torch

# å‡†å¤‡è¾“å…¥
B, S, n, D = 2, 128, 4, 256
x = torch.randn(B, S, n, D, dtype=torch.bfloat16)
phi = torch.randn(n*n + 2*n, n*D, dtype=torch.float32)
alpha = torch.tensor([1.1, 0.9, 1.05], dtype=torch.float32)
bias = torch.randn(n*n + 2*n, dtype=torch.float32) * 0.1

# å‰å‘ä¼ æ’­
h_in, h_post, h_res = mhc_forward_pre(x, phi, alpha, bias)

print(f"h_in shape: {h_in.shape}")      # [2, 128, 256]
print(f"h_post shape: {h_post.shape}")  # [2, 128, 4]
print(f"h_res shape: {h_res.shape}")    # [2, 128, 4, 4]
```

### 2. ä½¿ç”¨ Triton åŠ é€Ÿ

```python
from src.forward.mhc_forward_pre_triton import mhc_forward_pre_triton_optimized
import torch

# åœ¨ GPU ä¸Šè¿è¡Œ
device = 'cuda' if torch.cuda.is_available() else 'cpu'
x = torch.randn(B, S, n, D, dtype=torch.bfloat16, device=device)
phi = torch.randn(n*n + 2*n, n*D, dtype=torch.float32, device=device)
alpha = torch.tensor([1.1, 0.9, 1.05], device=device)
bias = torch.randn(n*n + 2*n, dtype=torch.float32, device=device) * 0.1

# å‰å‘ä¼ æ’­ (GPU åŠ é€Ÿ)
h_in, h_post, h_res = mhc_forward_pre_triton_optimized(x, phi, alpha, bias)
```

### 3. åå‘ä¼ æ’­ (Backward)

```python
from src.forward import mhc_forward_pre
from src.backward import mhc_backward_manual
import torch

# å‰å‘ä¼ æ’­ (éœ€è¦ä¸­é—´å€¼)
h_in, h_post, h_res, inv_rms, h_mix, h_pre = mhc_forward_pre(
    x, phi, alpha, bias, outflag=True
)

# å‡†å¤‡æ¢¯åº¦
dh_in = torch.randn_like(h_in)
dh_post = torch.randn_like(h_post)
dh_res = torch.randn_like(h_res)
gamma = torch.randn(n, D)

# åå‘ä¼ æ’­
dx, dphi, dalpha, dbias, dgamma = mhc_backward_manual(
    x, phi, alpha, bias,
    inv_rms, h_mix, h_pre, h_post,
    dh_in, dh_post, dh_res, gamma
)
```

---

## ç›®å½•ç»“æ„

```
mhc_ops/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ forward/                    # å‰å‘ä¼ æ’­å®ç°
â”‚   â”‚   â”œâ”€â”€ golden.py              # Golden å‚è€ƒå®ç°
â”‚   â”‚   â”œâ”€â”€ mhc_forward_pre_triton.py       # Triton GPU kernels
â”‚   â”‚   â””â”€â”€ mhc_forward_pre_tilelang.py     # TileLang DSL
â”‚   â”‚
â”‚   â”œâ”€â”€ backward/                   # åå‘ä¼ æ’­å®ç°
â”‚   â”‚   â”œâ”€â”€ golden.py              # Golden å‚è€ƒå®ç°
â”‚   â”‚   â”œâ”€â”€ mhc_backward_triton.py          # Triton (4-kernel æ¶æ„)
â”‚   â”‚   â””â”€â”€ mhc_backward_tilelang.py        # TileLang
â”‚   â”‚
â”‚   â””â”€â”€ __init__.py               # ç»Ÿä¸€å¯¼å‡º
â”‚
â”œâ”€â”€ test/
â”‚   â”œâ”€â”€ forward/                    # å‰å‘æµ‹è¯•
â”‚   â”‚   â”œâ”€â”€ test_forward.py       # å®Œæ•´æµ‹è¯•
â”‚   â”‚   â”œâ”€â”€ benchmark.py          # æ€§èƒ½åŸºå‡†æµ‹è¯•
â”‚   â”‚   â””â”€â”€ quick_test.py         # å¿«é€ŸéªŒè¯
â”‚   â”‚
â”‚   â””â”€â”€ backward/                   # åå‘æµ‹è¯•
â”‚       â””â”€â”€ test_backward.py      # Backward å®Œæ•´æµ‹è¯•
â”‚
â”œâ”€â”€ docs/                          # æ–‡æ¡£
â”‚   â””â”€â”€ BUGFIX_LOG.md             # Bug ä¿®å¤æ—¥å¿—
â”‚
â”œâ”€â”€ README.md                      # æœ¬æ–‡æ¡£
â”œâ”€â”€ QUICKSTART.md                  # å¿«é€Ÿå¼€å§‹æŒ‡å—
â”œâ”€â”€ CLAUDE.md                      # Claude Code é¡¹ç›®æŒ‡å—
â”œâ”€â”€ requirements.txt                # ä¾èµ–åˆ—è¡¨
â”œâ”€â”€ setup.py                       # å®‰è£…é…ç½®
â””â”€â”€ run_tests.sh                   # æµ‹è¯•è¿è¡Œè„šæœ¬
```

**ä¸»è¦å˜åŒ–:**
- âœ… æŒ‰ `forward/` å’Œ `backward/` é‡ç»„ç›®å½•ç»“æ„
- âœ… æ·»åŠ  BUGFIX_LOG.md è®°å½•ä¿®å¤è¿‡ç¨‹
- âœ… æ›´æ–°æµ‹è¯•è„šæœ¬ä»¥æ”¯æŒæ–°ç»“æ„

---

## ä½¿ç”¨ç¤ºä¾‹

### åŸºç¡€ä½¿ç”¨

```python
from src.forward import mhc_forward_pre

# è¾“å…¥
B, S, n, D = 1, 256, 4, 256
x = torch.randn(B, S, n, D, dtype=torch.bfloat16)
phi = torch.randn(n*n + 2*n, n*D, dtype=torch.float32)
alpha = torch.tensor([1.1, 0.9, 1.05], dtype=torch.float32)
bias = torch.randn(n*n + 2*n, dtype=torch.float32) * 0.1

# æ‰§è¡Œ
h_in, h_post, h_res = mhc_forward_pre(x, phi, alpha, bias)
```

### GPU åŠ é€Ÿ

```python
from src.forward.mhc_forward_pre_triton import mhc_forward_pre_triton_optimized

device = 'cuda'
x = torch.randn(B, S, n, D, dtype=torch.bfloat16, device=device)
phi = torch.randn(n*n + 2*n, n*D, dtype=torch.float32, device=device)
alpha = torch.tensor([1.1, 0.9, 1.05], device=device)
bias = torch.randn(n*n + 2*n, dtype=torch.float32, device=device) * 0.1

h_in, h_post, h_res = mhc_forward_pre_triton_optimized(x, phi, alpha, bias)
```

### å®Œæ•´çš„å‰å‘ + åå‘

```python
from src.forward import mhc_forward_pre
from src.backward import mhc_backward_manual

# å‰å‘
h_in, h_post, h_res, inv_rms, h_mix, h_pre = mhc_forward_pre(
    x, phi, alpha, bias, outflag=True
)

# è®¡ç®—æŸå¤±
loss = h_in.sum() + h_post.sum() + h_res.sum()

# åå‘
dh_in = torch.ones_like(h_in)
dh_post = torch.ones_like(h_post)
dh_res = torch.ones_like(h_res)

dx, dphi, dalpha, dbias, dgamma = mhc_backward_manual(
    x, phi, alpha, bias,
    inv_rms, h_mix, h_pre, h_post,
    dh_in, dh_post, dh_res, gamma
)
```

---

## API å‚è€ƒ

### Forward ç®—å­

#### `mhc_forward_pre(x, phi, alpha, bias, outflag=False, norm_eps=1e-6, hc_eps=1e-6)`

Golden å‚è€ƒå®ç°çš„å‰å‘ä¼ æ’­ã€‚

**å‚æ•°:**
- `x` ([B, S, n, D]): è¾“å…¥å¼ é‡ (BFloat16)
- `phi` ([nÂ²+2n, nD]): æƒé‡çŸ©é˜µ (Float32)
- `alpha` ([3]): ç¼©æ”¾å› å­ [pre, post, res] (Float32)
- `bias` ([nÂ²+2n]): åç½®å‘é‡ (Float32)
- `outflag` (bool): æ˜¯å¦è¿”å›ä¸­é—´å€¼ (ç”¨äºåå‘ä¼ æ’­)
- `norm_eps` (float): RMSNorm epsilon
- `hc_eps` (float): Hyper connection epsilon

**è¿”å›:**
- `h_in` ([B, S, D]): å‰ç½®é—¨æ§åŠ æƒè¾“å…¥ (BFloat16)
- `h_post` ([B, S, n]): åç½®é—¨æ§æ¿€æ´»å€¼ (Float32)
- `h_res` ([B, S, n, n]): æ®‹å·®é—¨æ§çŸ©é˜µ (Float32)
- å¦‚æœ `outflag=True`, é¢å¤–è¿”å›:
  - `inv_rms` ([B, S]): RMSNorm çš„é€†å‡æ–¹æ ¹
  - `h_mix` ([B, S, nÂ²+2n]): GEMM è¾“å‡ºï¼ˆå½’ä¸€åŒ–å‰ï¼‰
  - `h_pre` ([B, S, n]): Sigmoid æ¿€æ´»å‰çš„å€¼

#### `mhc_forward_pre_triton_optimized(x, phi, alpha, bias, outflag=False, norm_eps=1e-6, hc_eps=1e-6)`

Triton ä¼˜åŒ–ç‰ˆæœ¬çš„å‰å‘ä¼ æ’­ï¼Œæ€§èƒ½æ›´é«˜ã€‚

**å‚æ•°ä¸è¿”å›**: åŒ `mhc_forward_pre`

**æ€§èƒ½**: ç›¸æ¯” Golden å®ç°ï¼Œåœ¨ GPU ä¸Šæœ‰ 2-5x åŠ é€Ÿã€‚

### Backward ç®—å­

#### `mhc_backward_manual(x, phi, alpha, bias, inv_rms, h_mix, h_pre, h_post, dh_in, dh_post, dh_res, gamma, norm_eps=1e-6, hc_eps=1e-6)`

Golden å‚è€ƒå®ç°çš„åå‘ä¼ æ’­ã€‚

**å‚æ•°:**
- `x`, `phi`, `alpha`, `bias`: å‰å‘è¾“å…¥
- `inv_rms`, `h_mix`, `h_pre`, `h_post`: å‰å‘ä¸­é—´å€¼ (from `outflag=True`)
- `dh_in` ([B, S, D]): h_in çš„æ¢¯åº¦
- `dh_post` ([B, S, n]): h_post çš„æ¢¯åº¦
- `dh_res` ([B, S, n, n]): h_res çš„æ¢¯åº¦
- `gamma` ([n, D]): ç¼©æ”¾å› å­

**è¿”å›:**
- `dx` ([B, S, n, D]): x çš„æ¢¯åº¦ (BFloat16)
- `dphi` ([nÂ²+2n, nD]): phi çš„æ¢¯åº¦ (Float32)
- `dalpha` ([3]): alpha çš„æ¢¯åº¦ (Float32)
- `dbias` ([nÂ²+2n]): bias çš„æ¢¯åº¦ (Float32)
- `dgamma` ([n, D]): gamma çš„æ¢¯åº¦ (Float32)

#### `mhc_backward_triton(x, phi, alpha, bias, inv_rms, h_mix, h_pre, h_post, dh_in, dh_post, dh_res, gamma, norm_eps=1e-6, hc_eps=1e-6)`

Triton å®ç°çš„åå‘ä¼ æ’­ï¼Œä½¿ç”¨å¤š kernel æ¶æ„ã€‚

**å‚æ•°ä¸è¿”å›**: åŒ `mhc_backward_manual`

**æ¶æ„:**
- Kernel 1: è®¡ç®—ä¸»æ¢¯åº¦ (dalpha, dbias, dvecX_mm, dvecX_inv)
- Kernel 2: è®¡ç®— dx
- Kernel 3: è®¡ç®— dphi (âœ… å®Œå…¨æ­£ç¡®)
- Kernel 4: è®¡ç®— dgamma

**çŠ¶æ€**: éƒ¨åˆ†ç»„ä»¶æ­£ç¡®ï¼Œè¯¦è§ [å®ç°çŠ¶æ€](#å®ç°çŠ¶æ€)

---

---

## æµ‹è¯•

### å¿«é€Ÿæµ‹è¯•

```bash
# ä½¿ç”¨ conda ç¯å¢ƒè¿è¡Œæµ‹è¯•
conda run -n mhc_ops python test/forward/quick_test.py

# Forward æ€§èƒ½åŸºå‡†æµ‹è¯•
conda run -n mhc_ops python test/forward/benchmark.py

# Backward æµ‹è¯• (éƒ¨åˆ†ç»„ä»¶æ­£ç¡®)
conda run -n mhc_ops python test/backward/test_backward.py

# è¿è¡Œæ‰€æœ‰æµ‹è¯•
./run_tests.sh
```

### æµ‹è¯•é…ç½®

æ ‡å‡†æµ‹è¯•é…ç½®:
```python
(B, S, n, D) = [
    (2, 64, 4, 128),   # åŸºå‡†é…ç½®
    (2, 256, 4, 256),  # å¤§åºåˆ—ï¼Œå¤§ç»´åº¦
    (4, 512, 4, 512),  # å¤§æ‰¹æ¬¡
]
```

### é¢„æœŸç»“æœ

**Forward æµ‹è¯• (åŸºäºå®é™…æµ‹è¯•ç»“æœ):**
- âœ… Golden: åŸºå‡†å®ç°ï¼Œ100% æ­£ç¡®
- âš ï¸ **Triton: æ¥è¿‘é€šè¿‡ï¼Œä½†ç•¥å¾®è¶…å‡ºå®¹å·®**
  - `h_in`: max_err â‰ˆ 0.0156 (bfloat16 ç²¾åº¦èŒƒå›´ï¼Œç•¥å¾®è¶…å‡º rtol=1e-3)
  - `h_post`: max_err â‰ˆ 0.0001 (ä¼˜ç§€)
  - `h_res`: max_err â‰ˆ 0.013 (æ¥è¿‘å®¹å·®è¾¹ç•Œ)
  - **æ€§èƒ½**: 2-4x åŠ é€Ÿç›¸æ¯” Golden
  - **å»ºè®®**: å¯ç”¨äºç”Ÿäº§ç¯å¢ƒï¼Œä½†éœ€æ ¹æ®åº”ç”¨è°ƒæ•´å®¹å·®è¦æ±‚
- âŒ **TileLang: å·²ç¦ç”¨** (API ä¸å…¼å®¹ï¼Œæ— æ³•è¿è¡Œ)

**Backward æµ‹è¯• (åŸºäºå®é™…æµ‹è¯•ç»“æœ):**
- âœ… Golden: æ‰€æœ‰æ¢¯åº¦è®¡ç®—æ­£ç¡®
- âœ… **Triton: æ‰€æœ‰ç»„ä»¶å®Œå…¨æ­£ç¡®ï¼** (2025-02-25)
  - âœ… dphi: max_err < 1e-5
  - âœ… dalpha: max_err < 1e-4
  - âœ… dbias: max_err < 1e-5
  - âœ… dgamma: max_err < 1e-4
  - âœ… dx: max_err â‰ˆ 0.25 (bfloat16 ç²¾åº¦é™åˆ¶)
  - **æ€§èƒ½**: 0.74-0.86x vs Golden (å¯æ¥å—)
- âŒ **TileLang: å·²ç¦ç”¨** (API ä¸å…¼å®¹ï¼Œéœ€è¦å®Œå…¨é‡å†™)

---

## æ–‡æ¡£

### ç”¨æˆ·æŒ‡å—
- **[QUICKSTART.md](QUICKSTART.md)** - 5åˆ†é’Ÿå¿«é€Ÿä¸Šæ‰‹
- **[BACKWARD.md](BACKWARD.md)** - Backward ç®—å­è¯¦ç»†æ–‡æ¡£
- **[PROJECT_STRUCTURE.md](PROJECT_STRUCTURE.md)** - é¡¹ç›®ç»“æ„è¯´æ˜

### TileLang ç›¸å…³
- **[TILELANG_STATUS.md](docs/TILELANG_STATUS.md)** - TileLang å®ç°çŠ¶æ€æ€»ç»“
- **[TILELANG_BACKWARD_ISSUES.md](docs/TILELANG_BACKWARD_ISSUES.md)** - TileLang Backward è¯¦ç»†é—®é¢˜åˆ†æ
- **[TILELANG_REWRITE_PLAN.md](docs/TILELANG_REWRITE_PLAN.md)** - TileLang åŸç”Ÿ API é‡å†™è®¡åˆ’
- **[TILELANG_API_CHEATSHEET.md](docs/TILELANG_API_CHEATSHEET.md)** - TileLang API é€ŸæŸ¥è¡¨
- **[tilelang_knowledge_memory.json](docs/tilelang_knowledge_memory.json)** - TileLang çŸ¥è¯†åº“ï¼ˆç»“æ„åŒ– JSONï¼‰

### Bug ä¿®å¤æ—¥å¿—
- **[BUGFIX_LOG.md](BUGFIX_LOG.md)** - é—®é¢˜ä¿®å¤è®°å½•

---

## å¼•ç”¨

å¦‚æœæ‚¨åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº†è¿™äº›å®ç°ï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@software{mhc_ops,
  title={MHC Forward Pre Operator Implementations},
  author={Your Name},
  year={2025},
  url={https://github.com/folrent1896/mhc_ops}
}
```

---

## è®¸å¯è¯

è¯·å‚è€ƒä¸»ä»“åº“çš„è®¸å¯è¯ã€‚

---

## è”ç³»æ–¹å¼

- GitHub: [https://github.com/folrent1896/mhc_ops](https://github.com/folrent1896/mhc_ops)
- Issues: [æäº¤é—®é¢˜](https://github.com/folrent1896/mhc_ops/issues)
