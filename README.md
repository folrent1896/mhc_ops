# MHC Forward Pre ç®—å­å®ç°

å¤šç§åç«¯å®ç°çš„ `mhc_forward_pre` æµå½¢çº¦æŸè¶…è¿æ¥å‰ç½®ç®—å­ã€‚

---

## ğŸ“‹ ç›®å½•

- [æ¦‚è¿°](#æ¦‚è¿°)
- [ç‰¹æ€§](#ç‰¹æ€§)
- [å®‰è£…](#å®‰è£…)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [ç›®å½•ç»“æ„](#ç›®å½•ç»“æ„)
- [ä½¿ç”¨ç¤ºä¾‹](#ä½¿ç”¨ç¤ºä¾‹)
- [API å‚è€ƒ](#api-å‚è€ƒ)
- [æµ‹è¯•](#æµ‹è¯•)

---

## æ¦‚è¿°

æœ¬é¡¹ç›®æä¾›äº† `mhc_forward_pre` ç®—å­çš„å¤šç§é«˜æ€§èƒ½å®ç°ï¼š

| å®ç° | æè¿° | ä¼˜åŠ¿ |
|------|------|------|
| **Golden** | PyTorch å‚è€ƒå®ç° | éªŒè¯æ­£ç¡®æ€§çš„åŸºå‡† |
| **Triton** | GPU kernel å®ç° | é«˜æ€§èƒ½ GPU åŠ é€Ÿ |
| **TileLang** | DSL å¯ç§»æ¤å®ç° | è·¨å¹³å°ä¼˜åŒ– |

---

## ç‰¹æ€§

âœ¨ **å‰å‘ä¼ æ’­ (Forward)**
- GEMM çŸ©é˜µä¹˜æ³•
- RMSNorm å½’ä¸€åŒ–
- Sigmoid æ¿€æ´»å‡½æ•°
- æ”¯æŒå¯å˜æ‰¹æ¬¡å¤§å°å’Œåºåˆ—é•¿åº¦

âœ¨ **åå‘ä¼ æ’­ (Backward)**
- å®Œæ•´çš„æ¢¯åº¦è®¡ç®—
- æ”¯æŒ `dx`, `dphi`, `dalpha`, `dbias`, `dgamma`
- ä¸å‰å‘ä¼ æ’­æ— ç¼é›†æˆ

âœ¨ **å¤šç§åç«¯**
- **Golden**: çº¯ PyTorchï¼Œæ˜“äºè°ƒè¯•
- **Triton**: é«˜æ€§èƒ½ GPU kernel
- **TileLang**: è·¨å¹³å°å¯ç§»æ¤

---

## å®‰è£…

### ç¯å¢ƒè¦æ±‚

```bash
Python >= 3.8
CUDA >= 11.8 (å¯é€‰ï¼Œç”¨äº GPU åŠ é€Ÿ)
```

### å®‰è£…ä¾èµ–

```bash
# åŸºç¡€ä¾èµ–
pip install torch

# Triton (GPU åŠ é€Ÿ)
pip install triton

# TileLang (å¯é€‰)
pip install tilelang tvm
```

### ä»æºç å®‰è£…

```bash
git clone https://github.com/folrent1896/mhc_ops.git
cd mhc_ops

# å¼€å‘æ¨¡å¼å®‰è£…
pip install -e .
```

---

## å¿«é€Ÿå¼€å§‹

### 1. å‰å‘ä¼ æ’­ (Forward)

```python
from src.forward import mhc_forward_pre
import torch

# å‡†å¤‡è¾“å…¥
B, S, n, D = 2, 128, 4, 256
x = torch.randn(B, S, n, D, dtype=torch.bfloat16)
phi = torch.randn(n*n + 2*n, n*D, dtype=torch.float32)
alpha = torch.tensor([1.1, 0.9, 1.05], dtype=torch.float32)
bias = torch.randn(n*n + 2*n, dtype=torch.float32) * 0.1

# å‰å‘ä¼ æ’­
h_in, h_post, h_res = mhc_forward_pre(x, phi, alpha, bias)

print(f"h_in shape: {h_in.shape}")      # [2, 128, 256]
print(f"h_post shape: {h_post.shape}")  # [2, 128, 4]
print(f"h_res shape: {h_res.shape}")    # [2, 128, 4, 4]
```

### 2. ä½¿ç”¨ Triton åŠ é€Ÿ

```python
from src.forward.mhc_forward_pre_triton import mhc_forward_pre_triton_optimized
import torch

# åœ¨ GPU ä¸Šè¿è¡Œ
device = 'cuda' if torch.cuda.is_available() else 'cpu'
x = torch.randn(B, S, n, D, dtype=torch.bfloat16, device=device)
phi = torch.randn(n*n + 2*n, n*D, dtype=torch.float32, device=device)
alpha = torch.tensor([1.1, 0.9, 1.05], device=device)
bias = torch.randn(n*n + 2*n, dtype=torch.float32, device=device) * 0.1

# å‰å‘ä¼ æ’­ (GPU åŠ é€Ÿ)
h_in, h_post, h_res = mhc_forward_pre_triton_optimized(x, phi, alpha, bias)
```

### 3. åå‘ä¼ æ’­ (Backward)

```python
from src.forward import mhc_forward_pre
from src.backward import mhc_backward_manual
import torch

# å‰å‘ä¼ æ’­ (éœ€è¦ä¸­é—´å€¼)
h_in, h_post, h_res, inv_rms, h_mix, h_pre = mhc_forward_pre(
    x, phi, alpha, bias, outflag=True
)

# å‡†å¤‡æ¢¯åº¦
dh_in = torch.randn_like(h_in)
dh_post = torch.randn_like(h_post)
dh_res = torch.randn_like(h_res)
gamma = torch.randn(n, D)

# åå‘ä¼ æ’­
dx, dphi, dalpha, dbias, dgamma = mhc_backward_manual(
    x, phi, alpha, bias,
    inv_rms, h_mix, h_pre, h_post,
    dh_in, dh_post, dh_res, gamma
)
```

---

## ç›®å½•ç»“æ„

```
mhc-ops/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ forward/                    # å‰å‘ä¼ æ’­
â”‚   â”‚   â”œâ”€â”€ golden.py              # Golden å‚è€ƒ
â”‚   â”‚   â”œâ”€â”€ mhc_forward_pre_triton.py       # Triton
â”‚   â”‚   â””â”€â”€ mhc_forward_pre_tilelang.py     # TileLang
â”‚   â”‚
â”‚   â”œâ”€â”€ backward/                   # åå‘ä¼ æ’­
â”‚   â”‚   â”œâ”€â”€ golden.py              # Golden å‚è€ƒ
â”‚   â”‚   â”œâ”€â”€ mhc_backward_triton.py          # Triton
â”‚   â”‚   â””â”€â”€ mhc_backward_tilelang.py        # TileLang
â”‚   â”‚
â”‚   â””â”€â”€ __init__.py               # ç»Ÿä¸€å¯¼å‡º
â”‚
â”œâ”€â”€ test/
â”‚   â”œâ”€â”€ forward/                    # å‰å‘æµ‹è¯•
â”‚   â”‚   â”œâ”€â”€ test_forward.py       # å®Œæ•´æµ‹è¯•
â”‚   â”‚   â”œâ”€â”€ benchmark.py          # æ€§èƒ½æµ‹è¯•
â”‚   â”‚   â””â”€â”€ quick_test.py         # å¿«é€ŸéªŒè¯
â”‚   â”‚
â”‚   â””â”€â”€ backward/                   # åå‘æµ‹è¯•
â”‚       â””â”€â”€ test_backward.py      # Backward æµ‹è¯•
â”‚
â”œâ”€â”€ README.md                      # æœ¬æ–‡æ¡£
â”œâ”€â”€ QUICKSTART.md                  # å¿«é€Ÿå¼€å§‹
â”œâ”€â”€ BACKWARD.md                    # Backward æ–‡æ¡£
â”œâ”€â”€ PROJECT_STRUCTURE.md            # é¡¹ç›®ç»“æ„
â”œâ”€â”€ requirements.txt                # ä¾èµ–åˆ—è¡¨
â””â”€â”€ setup.py                       # å®‰è£…é…ç½®
```

---

## ä½¿ç”¨ç¤ºä¾‹

### åŸºç¡€ä½¿ç”¨

```python
from src.forward import mhc_forward_pre

# è¾“å…¥
B, S, n, D = 1, 256, 4, 256
x = torch.randn(B, S, n, D, dtype=torch.bfloat16)
phi = torch.randn(n*n + 2*n, n*D, dtype=torch.float32)
alpha = torch.tensor([1.1, 0.9, 1.05], dtype=torch.float32)
bias = torch.randn(n*n + 2*n, dtype=torch.float32) * 0.1

# æ‰§è¡Œ
h_in, h_post, h_res = mhc_forward_pre(x, phi, alpha, bias)
```

### GPU åŠ é€Ÿ

```python
from src.forward.mhc_forward_pre_triton import mhc_forward_pre_triton_optimized

device = 'cuda'
x = torch.randn(B, S, n, D, dtype=torch.bfloat16, device=device)
phi = torch.randn(n*n + 2*n, n*D, dtype=torch.float32, device=device)
alpha = torch.tensor([1.1, 0.9, 1.05], device=device)
bias = torch.randn(n*n + 2*n, dtype=torch.float32, device=device) * 0.1

h_in, h_post, h_res = mhc_forward_pre_triton_optimized(x, phi, alpha, bias)
```

### å®Œæ•´çš„å‰å‘ + åå‘

```python
from src.forward import mhc_forward_pre
from src.backward import mhc_backward_manual

# å‰å‘
h_in, h_post, h_res, inv_rms, h_mix, h_pre = mhc_forward_pre(
    x, phi, alpha, bias, outflag=True
)

# è®¡ç®—æŸå¤±
loss = h_in.sum() + h_post.sum() + h_res.sum()

# åå‘
dh_in = torch.ones_like(h_in)
dh_post = torch.ones_like(h_post)
dh_res = torch.ones_like(h_res)

dx, dphi, dalpha, dbias, dgamma = mhc_backward_manual(
    x, phi, alpha, bias,
    inv_rms, h_mix, h_pre, h_post,
    dh_in, dh_post, dh_res, gamma
)
```

---

## API å‚è€ƒ

### Forward ç®—å­

#### `mhc_forward_pre(x, phi, alpha, bias, outflag=False, norm_eps=1e-6, hc_eps=1e-6)`

Golden å‚è€ƒå®ç°çš„å‰å‘ä¼ æ’­ã€‚

**å‚æ•°:**
- `x` ([B, S, n, D]): è¾“å…¥å¼ é‡ (BFloat16)
- `phi` ([nÂ²+2n, nD]): æƒé‡çŸ©é˜µ (Float32)
- `alpha` ([3]): ç¼©æ”¾å› å­ [pre, post, res] (Float32)
- `bias` ([nÂ²+2n]): åç½®å‘é‡ (Float32)
- `outflag` (bool): æ˜¯å¦è¿”å›ä¸­é—´å€¼
- `norm_eps` (float): RMSNorm epsilon
- `hc_eps` (float): Hyper connection epsilon

**è¿”å›:**
- `h_in` ([B, S, D]): å‰ç½®é—¨æ§åŠ æƒè¾“å…¥ (BFloat16)
- `h_post` ([B, S, n]): åç½®é—¨æ§æ¿€æ´»å€¼ (Float32)
- `h_res` ([B, S, n, n]): æ®‹å·®é—¨æ§çŸ©é˜µ (Float32)

#### `mhc_forward_pre_triton_optimized(x, phi, alpha, bias, outflag=False, norm_eps=1e-6, hc_eps=1e-6)`

Triton ä¼˜åŒ–ç‰ˆæœ¬çš„å‰å‘ä¼ æ’­ï¼Œæ€§èƒ½æ›´é«˜ã€‚

**å‚æ•°ä¸è¿”å›**: åŒ `mhc_forward_pre`

### Backward ç®—å­

#### `mhc_backward_manual(x, phi, alpha, bias, inv_rms, h_mix, h_pre, h_post, dh_in, dh_post, dh_res, gamma, norm_eps=1e-6, hc_eps=1e-6)`

Golden å‚è€ƒå®ç°çš„åå‘ä¼ æ’­ã€‚

**å‚æ•°:**
- `x`, `phi`, `alpha`, `bias`: å‰å‘è¾“å…¥
- `inv_rms`, `h_mix`, `h_pre`, `h_post`: å‰å‘ä¸­é—´å€¼
- `dh_in`, `dh_post`, `dh_res`: è¾“å‡ºæ¢¯åº¦
- `gamma` ([n, D]): ç¼©æ”¾å› å­

**è¿”å›:**
- `dx` ([B, S, n, D]): x çš„æ¢¯åº¦
- `dphi` ([nÂ²+2n, nD]): phi çš„æ¢¯åº¦
- `dalpha` ([3]): alpha çš„æ¢¯åº¦
- `dbias` ([nÂ²+2n]): bias çš„æ¢¯åº¦
- `dgamma` ([n, D]): gamma çš„æ¢¯åº¦

---

---

## æµ‹è¯•

### å¿«é€Ÿæµ‹è¯•

```bash
# Forward å¿«é€Ÿæµ‹è¯•
python test/forward/quick_test.py

# Forward æ€§èƒ½æµ‹è¯•
python test/forward/benchmark.py

# Backward æµ‹è¯•
python test/backward/test_backward.py
```

### å®Œæ•´æµ‹è¯•å¥—ä»¶

```bash
# Forward å®Œæ•´æµ‹è¯•
python test/forward/test_forward.py --quick

# è‡ªå®šä¹‰é…ç½®
python test/forward/test_forward.py --device cuda --rtol 1e-4
```

---

## æ–‡æ¡£

- **[QUICKSTART.md](QUICKSTART.md)** - 5åˆ†é’Ÿå¿«é€Ÿä¸Šæ‰‹
- **[BACKWARD.md](BACKWARD.md)** - Backward ç®—å­è¯¦ç»†æ–‡æ¡£
- **[PROJECT_STRUCTURE.md](PROJECT_STRUCTURE.md)** - é¡¹ç›®ç»“æ„è¯´æ˜

---

## å¼•ç”¨

å¦‚æœæ‚¨åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº†è¿™äº›å®ç°ï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@software{mhc_ops,
  title={MHC Forward Pre Operator Implementations},
  author={Your Name},
  year={2025},
  url={https://github.com/folrent1896/mhc_ops}
}
```

---

## è®¸å¯è¯

è¯·å‚è€ƒä¸»ä»“åº“çš„è®¸å¯è¯ã€‚

---

## è”ç³»æ–¹å¼

- GitHub: [https://github.com/folrent1896/mhc_ops](https://github.com/folrent1896/mhc_ops)
- Issues: [æäº¤é—®é¢˜](https://github.com/folrent1896/mhc_ops/issues)
