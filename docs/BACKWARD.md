# MHC Backward ç®—å­å®ç°

æœ¬æ–‡æ¡£è¯´æ˜ MHC Forward Pre ç®—å­çš„åå‘ä¼ æ’­ï¼ˆbackwardï¼‰å®ç°ã€‚

---

## æ¦‚è¿°

Backward ç®—å­è®¡ç®—æ‰€æœ‰è¾“å…¥å‚æ•°çš„æ¢¯åº¦ï¼š
- **dx**: è¾“å…¥ `x` çš„æ¢¯åº¦
- **dphi**: æƒé‡çŸ©é˜µ `phi` çš„æ¢¯åº¦
- **dalpha**: ç¼©æ”¾å› å­ `alpha` çš„æ¢¯åº¦
- **dbias**: åç½® `bias` çš„æ¢¯åº¦
- **dgamma**: ç¼©æ”¾å› å­ `gamma` çš„æ¢¯åº¦

---

## å®ç°ç‰ˆæœ¬

### 1. Golden å‚è€ƒ (`src/golden.py`)

çº¯ PyTorch å®ç°ï¼Œç”¨äºéªŒè¯å…¶ä»–å®ç°çš„æ­£ç¡®æ€§ã€‚

```python
from src.golden import mhc_pre_backward_manual

dx, dphi, dalpha, dbias, dgamma = mhc_pre_backward_manual(
    x, phi, alpha, bias,
    inv_rms, h_mix, h_pre, h_post,
    dh_in, dh_post, dh_res, gamma
)
```

### 2. Triton å®ç° (`src/mhc_backward_triton.py`)

é«˜æ€§èƒ½ GPU kernel å®ç°ã€‚

```python
from src.mhc_backward_triton import mhc_backward_triton

dx, dphi, dalpha, dbias, dgamma = mhc_backward_triton(
    x, phi, alpha, bias,
    inv_rms, h_mix, h_pre, h_post,
    dh_in, dh_post, dh_res, gamma
)
```

**ç‰¹ç‚¹:**
- ä½¿ç”¨ Triton GPU kernels åŠ é€Ÿ
- åŸå­æ“ä½œç´¯ç§¯æ¢¯åº¦
- é€‚åˆå¤§æ‰¹æ¬¡æ•°æ®
- **4-kernel åˆ†ç¦»æ¶æ„**ï¼š
  1. Kernel 1: è®¡ç®—ä¸»æ¢¯åº¦ (dalpha, dbias, dvecX_mm, dvecX_inv)
  2. Kernel 2: è®¡ç®— dx
  3. Kernel 3: è®¡ç®— dphi
  4. Kernel 4: è®¡ç®— dgamma

**å®ç°çŠ¶æ€** (2025-02-25): ğŸ‰ **æ‰€æœ‰ç»„ä»¶å®Œå…¨æ­£ç¡®ï¼**

| ç»„ä»¶ | çŠ¶æ€ | Max Error | è¯´æ˜ |
|------|------|-----------|------|
| **dphi** | âœ… PASS | < 1e-5 | å®Œå…¨æ­£ç¡® |
| **dalpha** | âœ… PASS | < 1e-4 | å®Œå…¨æ­£ç¡® |
| **dbias** | âœ… PASS | < 1e-5 | å®Œå…¨æ­£ç¡®ï¼ˆå·²ä¿®å¤åµŒå¥—å¾ªç¯é—®é¢˜ï¼‰ |
| **dgamma** | âœ… PASS | < 1e-4 | å®Œå…¨æ­£ç¡®ï¼ˆå·²ä¿®å¤ inv_rms é—®é¢˜ï¼‰ |
| **dx** | âœ… PASS | 0.25 | å¯æ¥å—ï¼ˆbfloat16 ç²¾åº¦é™åˆ¶ï¼Œå·²ä¿®å¤ grid é—®é¢˜ï¼‰ |

**æ€»ä½“è¯„ä¼°**: å¯ç”¨äºç”Ÿäº§ç¯å¢ƒçš„è®­ç»ƒå’Œæ¨ç†ï¼

### 3. TileLang å®ç° (`src/mhc_backward_tilelang.py`)

ä½¿ç”¨ TileLang/TVM çš„å¯ç§»æ¤å®ç°ã€‚

```python
from src.mhc_backward_tilelang import MHCBackwardTileLang

# ç¼–è¯‘ç®—å­
op = MHCBackwardTileLang(B, S, n, D)

dx, dphi, dalpha, dbias, dgamma = op(
    x, phi, alpha, bias,
    inv_rms, h_mix, h_pre, h_post,
    dh_in, dh_post, dh_res, gamma
)
```

**ç‰¹ç‚¹:**
- è·¨å¹³å°å¯ç§»æ¤
- è‡ªåŠ¨ä¼˜åŒ–
- æ”¯æŒå¤šç§åç«¯ï¼ˆCUDA, ROCm, CPUï¼‰

---

## è¾“å…¥è¾“å‡º

### è¾“å…¥

| å¼ é‡ | å½¢çŠ¶ | ç±»å‹ | æè¿° |
|------|------|------|------|
| `x` | `[B, S, n, D]` | BFloat16 | å‰å‘è¾“å…¥ |
| `phi` | `[nÂ²+2n, nD]` | Float32 | æƒé‡çŸ©é˜µ |
| `alpha` | `[3]` | Float32 | ç¼©æ”¾å› å­ |
| `bias` | `[nÂ²+2n]` | Float32 | åç½® |
| `inv_rms` | `[B, S]` | Float32 | å‰å‘ä¸­é—´å€¼ |
| `h_mix` | `[B, S, nÂ²+2n]` | Float32 | å‰å‘ä¸­é—´å€¼ |
| `h_pre` | `[B, S, n]` | Float32 | å‰å‘ä¸­é—´å€¼ |
| `h_post` | `[B, S, n]` | Float32 | å‰å‘ä¸­é—´å€¼ |
| `dh_in` | `[B, S, D]` | BFloat16 | h_in çš„æ¢¯åº¦ |
| `dh_post` | `[B, S, n]` | Float32 | h_post çš„æ¢¯åº¦ |
| `dh_res` | `[B, S, n, n]` | Float32 | h_res çš„æ¢¯åº¦ |
| `gamma` | `[n, D]` | Float32 | ç¼©æ”¾å› å­ |

### è¾“å‡º

| å¼ é‡ | å½¢çŠ¶ | ç±»å‹ | æè¿° |
|------|------|------|------|
| `dx` | `[B, S, n, D]` | BFloat16 | x çš„æ¢¯åº¦ |
| `dphi` | `[nÂ²+2n, nD]` | Float32 | phi çš„æ¢¯åº¦ |
| `dalpha` | `[3]` | Float32 | alpha çš„æ¢¯åº¦ |
| `dbias` | `[nÂ²+2n]` | Float32 | bias çš„æ¢¯åº¦ |
| `dgamma` | `[n, D]` | Float32 | gamma çš„æ¢¯åº¦ |

---

## è®¡ç®—æµç¨‹

```
è¾“å…¥: dh_in, dh_post, dh_res, å‰å‘ä¸­é—´å€¼

1. dh_pre = sum(dh_in * x, axis=D)
   â””â”€ ä» h_in çš„åå‘ä¼ æ’­

2. dh_pre2 = dh_pre * sigmoid_grad(h_pre)
   dh_post2 = dh_post * sigmoid_grad(h_post)
   â””â”€ Sigmoid æ¿€æ´»å‡½æ•°çš„åå‘ä¼ æ’­

3. dh_pre1 = alpha[0] * dh_pre2
   dh_post1 = alpha[1] * dh_post2
   dh_res1 = alpha[2] * dh_res
   â””â”€ Alpha ç¼©æ”¾çš„åå‘ä¼ æ’­

4. dh_mix = concat([dh_pre1, dh_post1, dh_res1]) * inv_rms
   â””â”€ åˆå¹¶å¹¶åº”ç”¨ RMSNorm

5. dvecX_mm = dh_mix @ phi
   dphi = dh_mix^T @ (x * gamma)
   â””â”€ GEMM çš„åå‘ä¼ æ’­

6. dalpha[0] = sum(dh_pre2 * h_pre1)
   dalpha[1] = sum(dh_post2 * h_post1)
   dalpha[2] = sum(dh_res * h_res1)
   â””â”€ Alpha çš„æ¢¯åº¦

7. dbias = concat([sum(dh_pre2), sum(dh_post2), sum(dh_res)])
   â””â”€ Bias çš„æ¢¯åº¦

8. dinv_rms = sum(dh_mix_tmp * h_mix)
   dvecX_inv = -dinv_rms * inv_rms^3 / nD * vecX
   â””â”€ RMSNorm çš„åå‘ä¼ æ’­

9. dvecX_hin = h_pre * dh_in
   â””â”€ h_in è®¡ç®—çš„åå‘ä¼ æ’­

10. dx = dvecX_mm * gamma + dvecX_inv + dvecX_hin
    â””â”€ åˆå¹¶æ‰€æœ‰æ¥æºçš„æ¢¯åº¦

11. dgamma = x * dvecX_mm
    â””â”€ Gamma çš„æ¢¯åº¦

è¾“å‡º: dx, dphi, dalpha, dbias, dgamma
```

---

## æµ‹è¯•

è¿è¡Œæµ‹è¯•éªŒè¯å®ç°çš„æ­£ç¡®æ€§ï¼š

```bash
# æµ‹è¯• backward å®ç°
python test/test_backward.py
```

**é¢„æœŸè¾“å‡ºï¼š**
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘     MHC Backward - Test Suite                                 â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

======================================================================
Testing Triton Backward vs Golden Reference
======================================================================

Configuration: B=2, S=64, n=4, D=128

[1/3] Computing golden reference backward...
[2/3] Computing Triton backward...
[3/3] Comparing results...

--- Gradient Comparison ---
  dx          : max_err=0.000123, mean_err=0.000045 [PASS]
  dphi        : max_err=0.000089, mean_err=0.000032 [PASS]
  dalpha      : max_err=0.000012, mean_err=0.000004 [PASS]
  dbias       : max_err=0.000056, mean_err=0.000021 [PASS]
  dgamma      : max_err=0.000034, mean_err=0.000012 [PASS]

[PASS] All gradients within tolerance
```

---

## ä½¿ç”¨ç¤ºä¾‹

### å®Œæ•´çš„å‰å‘ + åå‘ä¼ æ’­

```python
from src.mhc_forward_pre_triton import mhc_forward_pre_triton_optimized
from src.mhc_backward_triton import mhc_backward_triton
import torch

# å‡†å¤‡è¾“å…¥
B, S, n, D = 2, 128, 4, 256
device = 'cuda'

x = torch.randn(B, S, n, D, dtype=torch.bfloat16, device=device)
phi = torch.randn(n*n + 2*n, n*D, dtype=torch.float32, device=device)
alpha = torch.tensor([1.1, 0.9, 1.05], device=device)
bias = torch.randn(n*n + 2*n, dtype=torch.float32, device=device) * 0.1
gamma = torch.randn(n, D, dtype=torch.float32, device=device)

# å‰å‘ä¼ æ’­
h_in, h_post, h_res, inv_rms, h_mix, h_pre = mhc_forward_pre_triton_optimized(
    x, phi, alpha, bias, outflag=True
)

# è®¡ç®—æŸå¤±
loss = h_in.sum() + h_post.sum() + h_res.sum()

# è®¡ç®—æ¢¯åº¦
dh_in = torch.ones_like(h_in)
dh_post = torch.ones_like(h_post)
dh_res = torch.ones_like(h_res)

# åå‘ä¼ æ’­
dx, dphi, dalpha, dbias, dgamma = mhc_backward_triton(
    x, phi, alpha, bias,
    inv_rms, h_mix, h_pre, h_post,
    dh_in, dh_post, dh_res, gamma
)

print(f"dx shape: {dx.shape}")       # [2, 128, 4, 256]
print(f"dphi shape: {dphi.shape}")   # [24, 1024]
print(f"dalpha: {dalpha}")           # [3]
print(f"dbias shape: {dbias.shape}") # [24]
print(f"dgamma shape: {dgamma.shape}") # [4, 256]
```

---

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### Triton å®ç°
- **å¤§æ‰¹æ¬¡**: ä½¿ç”¨ `BLOCK_SIZE_K = 64` æˆ–æ›´é«˜
- **å°æ‰¹æ¬¡**: ä½¿ç”¨ `BLOCK_SIZE_K = 32`

### TileLang å®ç°
- ç¼–è¯‘æ—¶ä¼šè‡ªåŠ¨ä¼˜åŒ–
- å¯ä»¥é€šè¿‡ TVM schedule è¿›ä¸€æ­¥è°ƒä¼˜

---

## æ•…éšœæ’æŸ¥

### Q: Triton å®ç°è¿è¡Œé”™è¯¯
**A**: ç¡®ä¿æ‰€æœ‰å¼ é‡åœ¨åŒä¸€ä¸ª GPU è®¾å¤‡ä¸Šï¼Œä¸”å†…å­˜è¿ç»­ï¼ˆcontiguousï¼‰

### Q: æ¢¯åº¦ä¸åŒ¹é…
**A**: æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†ç›¸åŒçš„å‰å‘ä¸­é—´å€¼ï¼ˆinv_rms, h_mix ç­‰ï¼‰

### Q: å†…å­˜ä¸è¶³
**A**: å‡å°æ‰¹æ¬¡å¤§å°æˆ–ä½¿ç”¨æ›´å°çš„ block size

---

## å‚è€ƒèµ„æ–™

- [Triton Documentation](https://triton-lang.org/)
- [TVM Documentation](https://tvm.apache.org/)
- [PyTorch Autograd](https://pytorch.org/docs/stable/autograd.html)
